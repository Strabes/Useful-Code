{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition with the Digits Dataset\n",
    "\n",
    "Author: Greg Strabel\n",
    "\n",
    "The purpose of this notebook is to demonstrate a few common machine learning algorithms for image recognition, in particular:\n",
    "    1. Gaussian Naive Bayes\n",
    "    2. Support Vector Classifier\n",
    "    3. Convolutional Neural Network\n",
    "    \n",
    "First we will take a look at some examples from the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHsAAACNCAYAAAAn1Xb5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nANARzR4AAACAjmj2AAAAAHTk/wFk5oY4KCAizAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e5aff44630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "plt.close()\n",
    "\n",
    "mlpFig, axes = plt.subplots(1,10,figsize = (20,6))\n",
    "for i in range(10):\n",
    "    ax = axes.ravel()[i]\n",
    "    ax.axis('off')\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Target: %i' % digits.target[i])\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "x_train = data[:n_samples // 2]\n",
    "y_train = digits.target[:n_samples // 2]\n",
    "\n",
    "x_test = data[n_samples // 2 :]\n",
    "y_test = digits.target[n_samples // 2 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Gaussian Naive Bayes Classifier: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.97        88\n",
      "          1       0.81      0.74      0.77        91\n",
      "          2       0.87      0.84      0.85        86\n",
      "          3       0.88      0.79      0.83        91\n",
      "          4       1.00      0.73      0.84        92\n",
      "          5       0.70      0.81      0.76        91\n",
      "          6       0.96      0.99      0.97        91\n",
      "          7       0.65      0.81      0.72        89\n",
      "          8       0.61      0.76      0.68        88\n",
      "          9       0.77      0.66      0.71        92\n",
      "\n",
      "avg / total       0.82      0.81      0.81       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "gnb_pred = gnb.predict(x_test)\n",
    "\n",
    "print(\"Classification Report for Gaussian Naive Bayes Classifier: \\n%s\\n\"\n",
    "      % (metrics.classification_report(y_test, gnb_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Support Vector Classifier: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99        88\n",
      "          1       0.99      0.97      0.98        91\n",
      "          2       0.99      0.99      0.99        86\n",
      "          3       0.98      0.87      0.92        91\n",
      "          4       0.99      0.96      0.97        92\n",
      "          5       0.95      0.97      0.96        91\n",
      "          6       0.99      0.99      0.99        91\n",
      "          7       0.96      0.99      0.97        89\n",
      "          8       0.94      1.00      0.97        88\n",
      "          9       0.93      0.98      0.95        92\n",
      "\n",
      "avg / total       0.97      0.97      0.97       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(gamma=0.001)\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "svc_pred = svc.predict(x_test)\n",
    "\n",
    "print(\"Classification Report for Support Vector Classifier: \\n%s\\n\"\n",
    "      % (metrics.classification_report(y_test, svc_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Finally, I train a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) with two convolution layers, max pooling and [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).\n",
    "\n",
    "The code for this network is adapted from [this](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 250\n",
    "\n",
    "img_rows, img_cols = 8, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 898 samples, validate on 899 samples\n",
      "Epoch 1/250\n",
      "898/898 [==============================] - 0s - loss: 3.2249 - acc: 0.1325 - val_loss: 2.0931 - val_acc: 0.2536\n",
      "Epoch 2/250\n",
      "898/898 [==============================] - 0s - loss: 2.3822 - acc: 0.2094 - val_loss: 1.7450 - val_acc: 0.4672\n",
      "Epoch 3/250\n",
      "898/898 [==============================] - 0s - loss: 1.9779 - acc: 0.2751 - val_loss: 1.5666 - val_acc: 0.6641\n",
      "Epoch 4/250\n",
      "898/898 [==============================] - 0s - loss: 1.7202 - acc: 0.4031 - val_loss: 1.3551 - val_acc: 0.7186\n",
      "Epoch 5/250\n",
      "898/898 [==============================] - 0s - loss: 1.5712 - acc: 0.4677 - val_loss: 1.1797 - val_acc: 0.7264\n",
      "Epoch 6/250\n",
      "898/898 [==============================] - 0s - loss: 1.3757 - acc: 0.5590 - val_loss: 1.0268 - val_acc: 0.7809\n",
      "Epoch 7/250\n",
      "898/898 [==============================] - 0s - loss: 1.2246 - acc: 0.6158 - val_loss: 0.8452 - val_acc: 0.8788\n",
      "Epoch 8/250\n",
      "898/898 [==============================] - 0s - loss: 1.1271 - acc: 0.6526 - val_loss: 0.7576 - val_acc: 0.8932\n",
      "Epoch 9/250\n",
      "898/898 [==============================] - 0s - loss: 1.0011 - acc: 0.6793 - val_loss: 0.6648 - val_acc: 0.9010\n",
      "Epoch 10/250\n",
      "898/898 [==============================] - 0s - loss: 0.9214 - acc: 0.7138 - val_loss: 0.6694 - val_acc: 0.8476\n",
      "Epoch 11/250\n",
      "898/898 [==============================] - 0s - loss: 0.9100 - acc: 0.7238 - val_loss: 0.5852 - val_acc: 0.8732\n",
      "Epoch 12/250\n",
      "898/898 [==============================] - 0s - loss: 0.8262 - acc: 0.7450 - val_loss: 0.5121 - val_acc: 0.9143\n",
      "Epoch 13/250\n",
      "898/898 [==============================] - 0s - loss: 0.7912 - acc: 0.7617 - val_loss: 0.4833 - val_acc: 0.9021\n",
      "Epoch 14/250\n",
      "898/898 [==============================] - 0s - loss: 0.7470 - acc: 0.7739 - val_loss: 0.4695 - val_acc: 0.9088\n",
      "Epoch 15/250\n",
      "898/898 [==============================] - 0s - loss: 0.6944 - acc: 0.7940 - val_loss: 0.4221 - val_acc: 0.9077\n",
      "Epoch 16/250\n",
      "898/898 [==============================] - 0s - loss: 0.6217 - acc: 0.8408 - val_loss: 0.4347 - val_acc: 0.9010\n",
      "Epoch 17/250\n",
      "898/898 [==============================] - 0s - loss: 0.6513 - acc: 0.8107 - val_loss: 0.4032 - val_acc: 0.9199\n",
      "Epoch 18/250\n",
      "898/898 [==============================] - 0s - loss: 0.5838 - acc: 0.8341 - val_loss: 0.3584 - val_acc: 0.9232\n",
      "Epoch 19/250\n",
      "898/898 [==============================] - 0s - loss: 0.5807 - acc: 0.8085 - val_loss: 0.3487 - val_acc: 0.9166\n",
      "Epoch 20/250\n",
      "898/898 [==============================] - 0s - loss: 0.5475 - acc: 0.8452 - val_loss: 0.3344 - val_acc: 0.9277\n",
      "Epoch 21/250\n",
      "898/898 [==============================] - 0s - loss: 0.5166 - acc: 0.8474 - val_loss: 0.3489 - val_acc: 0.9299\n",
      "Epoch 22/250\n",
      "898/898 [==============================] - 0s - loss: 0.5407 - acc: 0.8463 - val_loss: 0.3331 - val_acc: 0.9210\n",
      "Epoch 23/250\n",
      "898/898 [==============================] - 0s - loss: 0.4647 - acc: 0.8697 - val_loss: 0.3175 - val_acc: 0.9299\n",
      "Epoch 24/250\n",
      "898/898 [==============================] - 0s - loss: 0.4668 - acc: 0.8719 - val_loss: 0.2999 - val_acc: 0.9344\n",
      "Epoch 25/250\n",
      "898/898 [==============================] - 0s - loss: 0.4435 - acc: 0.8697 - val_loss: 0.2880 - val_acc: 0.9355\n",
      "Epoch 26/250\n",
      "898/898 [==============================] - 0s - loss: 0.4481 - acc: 0.8797 - val_loss: 0.2895 - val_acc: 0.9266\n",
      "Epoch 27/250\n",
      "898/898 [==============================] - 0s - loss: 0.4178 - acc: 0.8686 - val_loss: 0.2830 - val_acc: 0.9310\n",
      "Epoch 28/250\n",
      "898/898 [==============================] - 0s - loss: 0.4487 - acc: 0.8530 - val_loss: 0.2711 - val_acc: 0.9333\n",
      "Epoch 29/250\n",
      "898/898 [==============================] - 0s - loss: 0.3946 - acc: 0.8886 - val_loss: 0.2522 - val_acc: 0.9355\n",
      "Epoch 30/250\n",
      "898/898 [==============================] - 0s - loss: 0.4158 - acc: 0.8797 - val_loss: 0.2481 - val_acc: 0.9377\n",
      "Epoch 31/250\n",
      "898/898 [==============================] - 0s - loss: 0.3627 - acc: 0.8987 - val_loss: 0.2393 - val_acc: 0.9377\n",
      "Epoch 32/250\n",
      "898/898 [==============================] - 0s - loss: 0.3640 - acc: 0.8987 - val_loss: 0.2378 - val_acc: 0.9377\n",
      "Epoch 33/250\n",
      "898/898 [==============================] - 0s - loss: 0.3630 - acc: 0.8931 - val_loss: 0.2339 - val_acc: 0.9422\n",
      "Epoch 34/250\n",
      "898/898 [==============================] - 0s - loss: 0.3563 - acc: 0.8853 - val_loss: 0.2205 - val_acc: 0.9410\n",
      "Epoch 35/250\n",
      "898/898 [==============================] - 0s - loss: 0.3090 - acc: 0.9065 - val_loss: 0.2290 - val_acc: 0.9422\n",
      "Epoch 36/250\n",
      "898/898 [==============================] - 0s - loss: 0.3502 - acc: 0.9065 - val_loss: 0.2552 - val_acc: 0.9366\n",
      "Epoch 37/250\n",
      "898/898 [==============================] - 0s - loss: 0.3080 - acc: 0.9254 - val_loss: 0.2231 - val_acc: 0.9388\n",
      "Epoch 38/250\n",
      "898/898 [==============================] - 0s - loss: 0.3140 - acc: 0.9098 - val_loss: 0.2220 - val_acc: 0.9388\n",
      "Epoch 39/250\n",
      "898/898 [==============================] - 0s - loss: 0.3057 - acc: 0.9042 - val_loss: 0.2137 - val_acc: 0.9388\n",
      "Epoch 40/250\n",
      "898/898 [==============================] - 0s - loss: 0.2851 - acc: 0.9243 - val_loss: 0.2106 - val_acc: 0.9399\n",
      "Epoch 41/250\n",
      "898/898 [==============================] - 0s - loss: 0.2819 - acc: 0.9154 - val_loss: 0.2027 - val_acc: 0.9399\n",
      "Epoch 42/250\n",
      "898/898 [==============================] - 0s - loss: 0.2589 - acc: 0.9421 - val_loss: 0.1975 - val_acc: 0.9388\n",
      "Epoch 43/250\n",
      "898/898 [==============================] - 0s - loss: 0.2741 - acc: 0.9232 - val_loss: 0.1926 - val_acc: 0.9422\n",
      "Epoch 44/250\n",
      "898/898 [==============================] - 0s - loss: 0.2545 - acc: 0.9298 - val_loss: 0.1932 - val_acc: 0.9388\n",
      "Epoch 45/250\n",
      "898/898 [==============================] - 0s - loss: 0.2500 - acc: 0.9232 - val_loss: 0.1900 - val_acc: 0.9433\n",
      "Epoch 46/250\n",
      "898/898 [==============================] - 0s - loss: 0.2343 - acc: 0.9298 - val_loss: 0.1904 - val_acc: 0.9422\n",
      "Epoch 47/250\n",
      "898/898 [==============================] - 0s - loss: 0.2555 - acc: 0.9232 - val_loss: 0.1994 - val_acc: 0.9444\n",
      "Epoch 48/250\n",
      "898/898 [==============================] - 0s - loss: 0.2454 - acc: 0.9298 - val_loss: 0.1887 - val_acc: 0.9444\n",
      "Epoch 49/250\n",
      "898/898 [==============================] - 0s - loss: 0.2481 - acc: 0.9109 - val_loss: 0.1789 - val_acc: 0.9466\n",
      "Epoch 50/250\n",
      "898/898 [==============================] - 0s - loss: 0.2241 - acc: 0.9365 - val_loss: 0.1749 - val_acc: 0.9422\n",
      "Epoch 51/250\n",
      "898/898 [==============================] - 0s - loss: 0.2259 - acc: 0.9287 - val_loss: 0.1717 - val_acc: 0.9444\n",
      "Epoch 52/250\n",
      "898/898 [==============================] - 0s - loss: 0.2274 - acc: 0.9276 - val_loss: 0.1697 - val_acc: 0.9455\n",
      "Epoch 53/250\n",
      "898/898 [==============================] - 0s - loss: 0.2040 - acc: 0.9376 - val_loss: 0.1666 - val_acc: 0.9444\n",
      "Epoch 54/250\n",
      "898/898 [==============================] - 0s - loss: 0.1882 - acc: 0.9499 - val_loss: 0.1659 - val_acc: 0.9422\n",
      "Epoch 55/250\n",
      "898/898 [==============================] - 0s - loss: 0.1678 - acc: 0.9521 - val_loss: 0.1648 - val_acc: 0.9433\n",
      "Epoch 56/250\n",
      "898/898 [==============================] - 0s - loss: 0.1766 - acc: 0.9421 - val_loss: 0.1639 - val_acc: 0.9466\n",
      "Epoch 57/250\n",
      "898/898 [==============================] - 0s - loss: 0.1845 - acc: 0.9499 - val_loss: 0.1694 - val_acc: 0.9511\n",
      "Epoch 58/250\n",
      "898/898 [==============================] - 0s - loss: 0.1995 - acc: 0.9376 - val_loss: 0.1597 - val_acc: 0.9455\n",
      "Epoch 59/250\n",
      "898/898 [==============================] - 0s - loss: 0.1885 - acc: 0.9421 - val_loss: 0.1608 - val_acc: 0.9477\n",
      "Epoch 60/250\n",
      "898/898 [==============================] - 0s - loss: 0.1732 - acc: 0.9555 - val_loss: 0.1487 - val_acc: 0.9511\n",
      "Epoch 61/250\n",
      "898/898 [==============================] - 0s - loss: 0.1728 - acc: 0.9499 - val_loss: 0.1522 - val_acc: 0.9477\n",
      "Epoch 62/250\n",
      "898/898 [==============================] - 0s - loss: 0.1765 - acc: 0.9477 - val_loss: 0.1535 - val_acc: 0.9488\n",
      "Epoch 63/250\n",
      "898/898 [==============================] - 0s - loss: 0.1443 - acc: 0.9543 - val_loss: 0.1564 - val_acc: 0.9466\n",
      "Epoch 64/250\n",
      "898/898 [==============================] - 0s - loss: 0.1597 - acc: 0.9577 - val_loss: 0.1537 - val_acc: 0.9455\n",
      "Epoch 65/250\n",
      "898/898 [==============================] - 0s - loss: 0.1515 - acc: 0.9599 - val_loss: 0.1667 - val_acc: 0.9399\n",
      "Epoch 66/250\n",
      "898/898 [==============================] - 0s - loss: 0.1672 - acc: 0.9488 - val_loss: 0.1743 - val_acc: 0.9466\n",
      "Epoch 67/250\n",
      "898/898 [==============================] - 0s - loss: 0.1487 - acc: 0.9566 - val_loss: 0.1546 - val_acc: 0.9477\n",
      "Epoch 68/250\n",
      "898/898 [==============================] - 0s - loss: 0.1468 - acc: 0.9633 - val_loss: 0.1502 - val_acc: 0.9477\n",
      "Epoch 69/250\n",
      "898/898 [==============================] - 0s - loss: 0.1314 - acc: 0.9688 - val_loss: 0.1456 - val_acc: 0.9522\n",
      "Epoch 70/250\n",
      "898/898 [==============================] - 0s - loss: 0.1517 - acc: 0.9488 - val_loss: 0.1434 - val_acc: 0.9522\n",
      "Epoch 71/250\n",
      "898/898 [==============================] - 0s - loss: 0.1277 - acc: 0.9644 - val_loss: 0.1381 - val_acc: 0.9544\n",
      "Epoch 72/250\n",
      "898/898 [==============================] - 0s - loss: 0.1403 - acc: 0.9633 - val_loss: 0.1524 - val_acc: 0.9466\n",
      "Epoch 73/250\n",
      "898/898 [==============================] - 0s - loss: 0.1547 - acc: 0.9499 - val_loss: 0.1440 - val_acc: 0.9466\n",
      "Epoch 74/250\n",
      "898/898 [==============================] - 0s - loss: 0.1410 - acc: 0.9566 - val_loss: 0.1383 - val_acc: 0.9511\n",
      "Epoch 75/250\n",
      "898/898 [==============================] - 0s - loss: 0.1242 - acc: 0.9677 - val_loss: 0.1356 - val_acc: 0.9488\n",
      "Epoch 76/250\n",
      "898/898 [==============================] - 0s - loss: 0.1204 - acc: 0.9610 - val_loss: 0.1404 - val_acc: 0.9533\n",
      "Epoch 77/250\n",
      "898/898 [==============================] - 0s - loss: 0.1180 - acc: 0.9688 - val_loss: 0.1506 - val_acc: 0.9522\n",
      "Epoch 78/250\n",
      "898/898 [==============================] - 0s - loss: 0.1090 - acc: 0.9722 - val_loss: 0.1392 - val_acc: 0.9522\n",
      "Epoch 79/250\n",
      "898/898 [==============================] - 0s - loss: 0.1268 - acc: 0.9633 - val_loss: 0.1344 - val_acc: 0.9544\n",
      "Epoch 80/250\n",
      "898/898 [==============================] - 0s - loss: 0.1160 - acc: 0.9677 - val_loss: 0.1417 - val_acc: 0.9511\n",
      "Epoch 81/250\n",
      "898/898 [==============================] - 0s - loss: 0.1209 - acc: 0.9677 - val_loss: 0.1302 - val_acc: 0.9544\n",
      "Epoch 82/250\n",
      "898/898 [==============================] - 0s - loss: 0.1061 - acc: 0.9755 - val_loss: 0.1349 - val_acc: 0.9555\n",
      "Epoch 83/250\n",
      "898/898 [==============================] - 0s - loss: 0.0998 - acc: 0.9722 - val_loss: 0.1345 - val_acc: 0.9522\n",
      "Epoch 84/250\n",
      "898/898 [==============================] - 0s - loss: 0.1163 - acc: 0.9699 - val_loss: 0.1348 - val_acc: 0.9522\n",
      "Epoch 85/250\n",
      "898/898 [==============================] - 0s - loss: 0.0957 - acc: 0.9777 - val_loss: 0.1351 - val_acc: 0.9533\n",
      "Epoch 86/250\n",
      "898/898 [==============================] - 0s - loss: 0.1128 - acc: 0.9610 - val_loss: 0.1340 - val_acc: 0.9533\n",
      "Epoch 87/250\n",
      "898/898 [==============================] - 0s - loss: 0.1119 - acc: 0.9677 - val_loss: 0.1343 - val_acc: 0.9522\n",
      "Epoch 88/250\n",
      "898/898 [==============================] - 0s - loss: 0.0995 - acc: 0.9699 - val_loss: 0.1317 - val_acc: 0.9544\n",
      "Epoch 89/250\n",
      "898/898 [==============================] - 0s - loss: 0.1240 - acc: 0.9599 - val_loss: 0.1285 - val_acc: 0.9544\n",
      "Epoch 90/250\n",
      "898/898 [==============================] - 0s - loss: 0.0979 - acc: 0.9722 - val_loss: 0.1386 - val_acc: 0.9533\n",
      "Epoch 91/250\n",
      "898/898 [==============================] - 0s - loss: 0.1003 - acc: 0.9777 - val_loss: 0.1294 - val_acc: 0.9566\n",
      "Epoch 92/250\n",
      "898/898 [==============================] - 0s - loss: 0.0958 - acc: 0.9755 - val_loss: 0.1187 - val_acc: 0.9622\n",
      "Epoch 93/250\n",
      "898/898 [==============================] - 0s - loss: 0.0835 - acc: 0.9744 - val_loss: 0.1187 - val_acc: 0.9622\n",
      "Epoch 94/250\n",
      "898/898 [==============================] - 0s - loss: 0.0998 - acc: 0.9766 - val_loss: 0.1171 - val_acc: 0.9622\n",
      "Epoch 95/250\n",
      "898/898 [==============================] - 0s - loss: 0.0871 - acc: 0.9733 - val_loss: 0.1200 - val_acc: 0.9633\n",
      "Epoch 96/250\n",
      "898/898 [==============================] - 0s - loss: 0.0940 - acc: 0.9722 - val_loss: 0.1219 - val_acc: 0.9600\n",
      "Epoch 97/250\n",
      "898/898 [==============================] - 0s - loss: 0.0878 - acc: 0.9710 - val_loss: 0.1231 - val_acc: 0.9577\n",
      "Epoch 98/250\n",
      "898/898 [==============================] - 0s - loss: 0.0933 - acc: 0.9777 - val_loss: 0.1220 - val_acc: 0.9555\n",
      "Epoch 99/250\n",
      "898/898 [==============================] - 0s - loss: 0.0892 - acc: 0.9744 - val_loss: 0.1234 - val_acc: 0.9544\n",
      "Epoch 100/250\n",
      "898/898 [==============================] - 0s - loss: 0.0802 - acc: 0.9766 - val_loss: 0.1247 - val_acc: 0.9577\n",
      "Epoch 101/250\n",
      "898/898 [==============================] - 0s - loss: 0.0879 - acc: 0.9777 - val_loss: 0.1233 - val_acc: 0.9577\n",
      "Epoch 102/250\n",
      "898/898 [==============================] - 0s - loss: 0.0752 - acc: 0.9833 - val_loss: 0.1217 - val_acc: 0.9588\n",
      "Epoch 103/250\n",
      "898/898 [==============================] - 0s - loss: 0.0772 - acc: 0.9788 - val_loss: 0.1228 - val_acc: 0.9544\n",
      "Epoch 104/250\n",
      "898/898 [==============================] - 0s - loss: 0.0696 - acc: 0.9788 - val_loss: 0.1211 - val_acc: 0.9577\n",
      "Epoch 105/250\n",
      "898/898 [==============================] - 0s - loss: 0.0785 - acc: 0.9733 - val_loss: 0.1181 - val_acc: 0.9622\n",
      "Epoch 106/250\n",
      "898/898 [==============================] - 0s - loss: 0.0616 - acc: 0.9844 - val_loss: 0.1384 - val_acc: 0.9555\n",
      "Epoch 107/250\n",
      "898/898 [==============================] - 0s - loss: 0.0961 - acc: 0.9722 - val_loss: 0.1224 - val_acc: 0.9588\n",
      "Epoch 108/250\n",
      "898/898 [==============================] - 0s - loss: 0.0782 - acc: 0.9766 - val_loss: 0.1264 - val_acc: 0.9577\n",
      "Epoch 109/250\n",
      "898/898 [==============================] - 0s - loss: 0.0675 - acc: 0.9833 - val_loss: 0.1223 - val_acc: 0.9633\n",
      "Epoch 110/250\n",
      "898/898 [==============================] - 0s - loss: 0.0688 - acc: 0.9777 - val_loss: 0.1251 - val_acc: 0.9577\n",
      "Epoch 111/250\n",
      "898/898 [==============================] - 0s - loss: 0.0644 - acc: 0.9777 - val_loss: 0.1168 - val_acc: 0.9622\n",
      "Epoch 112/250\n",
      "898/898 [==============================] - 0s - loss: 0.0590 - acc: 0.9844 - val_loss: 0.1123 - val_acc: 0.9655\n",
      "Epoch 113/250\n",
      "898/898 [==============================] - 0s - loss: 0.0598 - acc: 0.9844 - val_loss: 0.1144 - val_acc: 0.9633\n",
      "Epoch 114/250\n",
      "898/898 [==============================] - 0s - loss: 0.0751 - acc: 0.9800 - val_loss: 0.1155 - val_acc: 0.9588\n",
      "Epoch 115/250\n",
      "898/898 [==============================] - 0s - loss: 0.0729 - acc: 0.9788 - val_loss: 0.1210 - val_acc: 0.9577\n",
      "Epoch 116/250\n",
      "898/898 [==============================] - 0s - loss: 0.0679 - acc: 0.9744 - val_loss: 0.1101 - val_acc: 0.9644\n",
      "Epoch 117/250\n",
      "898/898 [==============================] - 0s - loss: 0.0756 - acc: 0.9777 - val_loss: 0.1167 - val_acc: 0.9655\n",
      "Epoch 118/250\n",
      "898/898 [==============================] - 0s - loss: 0.0491 - acc: 0.9878 - val_loss: 0.1126 - val_acc: 0.9633\n",
      "Epoch 119/250\n",
      "898/898 [==============================] - 0s - loss: 0.0615 - acc: 0.9833 - val_loss: 0.1115 - val_acc: 0.9611\n",
      "Epoch 120/250\n",
      "898/898 [==============================] - 0s - loss: 0.0473 - acc: 0.9900 - val_loss: 0.1144 - val_acc: 0.9588\n",
      "Epoch 121/250\n",
      "898/898 [==============================] - 0s - loss: 0.0542 - acc: 0.9811 - val_loss: 0.1076 - val_acc: 0.9677\n",
      "Epoch 122/250\n",
      "898/898 [==============================] - 0s - loss: 0.0611 - acc: 0.9822 - val_loss: 0.1094 - val_acc: 0.9666\n",
      "Epoch 123/250\n",
      "898/898 [==============================] - 0s - loss: 0.0494 - acc: 0.9866 - val_loss: 0.1078 - val_acc: 0.9666\n",
      "Epoch 124/250\n",
      "898/898 [==============================] - 0s - loss: 0.0455 - acc: 0.9889 - val_loss: 0.1137 - val_acc: 0.9655\n",
      "Epoch 125/250\n",
      "898/898 [==============================] - 0s - loss: 0.0591 - acc: 0.9855 - val_loss: 0.1266 - val_acc: 0.9622\n",
      "Epoch 126/250\n",
      "898/898 [==============================] - 0s - loss: 0.0636 - acc: 0.9811 - val_loss: 0.1044 - val_acc: 0.9711\n",
      "Epoch 127/250\n",
      "898/898 [==============================] - 0s - loss: 0.0592 - acc: 0.9833 - val_loss: 0.1004 - val_acc: 0.9711\n",
      "Epoch 128/250\n",
      "898/898 [==============================] - 0s - loss: 0.0472 - acc: 0.9866 - val_loss: 0.1016 - val_acc: 0.9711\n",
      "Epoch 129/250\n",
      "898/898 [==============================] - 0s - loss: 0.0541 - acc: 0.9844 - val_loss: 0.1024 - val_acc: 0.9700\n",
      "Epoch 130/250\n",
      "898/898 [==============================] - 0s - loss: 0.0408 - acc: 0.9933 - val_loss: 0.1055 - val_acc: 0.9700\n",
      "Epoch 131/250\n",
      "898/898 [==============================] - 0s - loss: 0.0515 - acc: 0.9833 - val_loss: 0.1092 - val_acc: 0.9677\n",
      "Epoch 132/250\n",
      "898/898 [==============================] - 0s - loss: 0.0475 - acc: 0.9889 - val_loss: 0.1006 - val_acc: 0.9700\n",
      "Epoch 133/250\n",
      "898/898 [==============================] - 0s - loss: 0.0523 - acc: 0.9889 - val_loss: 0.1114 - val_acc: 0.9644\n",
      "Epoch 134/250\n",
      "898/898 [==============================] - 0s - loss: 0.0556 - acc: 0.9833 - val_loss: 0.1030 - val_acc: 0.9700\n",
      "Epoch 135/250\n",
      "898/898 [==============================] - 0s - loss: 0.0462 - acc: 0.9878 - val_loss: 0.1044 - val_acc: 0.9700\n",
      "Epoch 136/250\n",
      "898/898 [==============================] - 0s - loss: 0.0386 - acc: 0.9933 - val_loss: 0.1035 - val_acc: 0.9711\n",
      "Epoch 137/250\n",
      "898/898 [==============================] - 0s - loss: 0.0471 - acc: 0.9866 - val_loss: 0.1039 - val_acc: 0.9689\n",
      "Epoch 138/250\n",
      "898/898 [==============================] - 0s - loss: 0.0458 - acc: 0.9889 - val_loss: 0.1029 - val_acc: 0.9700\n",
      "Epoch 139/250\n",
      "898/898 [==============================] - 0s - loss: 0.0558 - acc: 0.9855 - val_loss: 0.1041 - val_acc: 0.9689\n",
      "Epoch 140/250\n",
      "898/898 [==============================] - 0s - loss: 0.0406 - acc: 0.9855 - val_loss: 0.1016 - val_acc: 0.9689\n",
      "Epoch 141/250\n",
      "898/898 [==============================] - 0s - loss: 0.0458 - acc: 0.9900 - val_loss: 0.1044 - val_acc: 0.9666\n",
      "Epoch 142/250\n",
      "898/898 [==============================] - 0s - loss: 0.0484 - acc: 0.9878 - val_loss: 0.0998 - val_acc: 0.9677\n",
      "Epoch 143/250\n",
      "898/898 [==============================] - 0s - loss: 0.0486 - acc: 0.9866 - val_loss: 0.1015 - val_acc: 0.9689\n",
      "Epoch 144/250\n",
      "898/898 [==============================] - 0s - loss: 0.0426 - acc: 0.9889 - val_loss: 0.0997 - val_acc: 0.9722\n",
      "Epoch 145/250\n",
      "898/898 [==============================] - 0s - loss: 0.0433 - acc: 0.9911 - val_loss: 0.1015 - val_acc: 0.9711\n",
      "Epoch 146/250\n",
      "898/898 [==============================] - 0s - loss: 0.0310 - acc: 0.9955 - val_loss: 0.1029 - val_acc: 0.9711\n",
      "Epoch 147/250\n",
      "898/898 [==============================] - 0s - loss: 0.0369 - acc: 0.9889 - val_loss: 0.0997 - val_acc: 0.9711\n",
      "Epoch 148/250\n",
      "898/898 [==============================] - 0s - loss: 0.0309 - acc: 0.9933 - val_loss: 0.1033 - val_acc: 0.9689\n",
      "Epoch 149/250\n",
      "898/898 [==============================] - 0s - loss: 0.0424 - acc: 0.9855 - val_loss: 0.1124 - val_acc: 0.9711\n",
      "Epoch 150/250\n",
      "898/898 [==============================] - 0s - loss: 0.0406 - acc: 0.9866 - val_loss: 0.1053 - val_acc: 0.9666\n",
      "Epoch 151/250\n",
      "898/898 [==============================] - 0s - loss: 0.0343 - acc: 0.9900 - val_loss: 0.1106 - val_acc: 0.9677\n",
      "Epoch 152/250\n",
      "898/898 [==============================] - 0s - loss: 0.0259 - acc: 0.9967 - val_loss: 0.1062 - val_acc: 0.9711\n",
      "Epoch 153/250\n",
      "898/898 [==============================] - 0s - loss: 0.0288 - acc: 0.9955 - val_loss: 0.1035 - val_acc: 0.9700\n",
      "Epoch 154/250\n",
      "898/898 [==============================] - 0s - loss: 0.0265 - acc: 0.9978 - val_loss: 0.0986 - val_acc: 0.9722\n",
      "Epoch 155/250\n",
      "898/898 [==============================] - 0s - loss: 0.0273 - acc: 0.9944 - val_loss: 0.0975 - val_acc: 0.9711\n",
      "Epoch 156/250\n",
      "898/898 [==============================] - 0s - loss: 0.0419 - acc: 0.9866 - val_loss: 0.1121 - val_acc: 0.9633\n",
      "Epoch 157/250\n",
      "898/898 [==============================] - 0s - loss: 0.0289 - acc: 0.9900 - val_loss: 0.1030 - val_acc: 0.9689\n",
      "Epoch 158/250\n",
      "898/898 [==============================] - 0s - loss: 0.0384 - acc: 0.9911 - val_loss: 0.0983 - val_acc: 0.9733\n",
      "Epoch 159/250\n",
      "898/898 [==============================] - 0s - loss: 0.0231 - acc: 0.9955 - val_loss: 0.1069 - val_acc: 0.9700\n",
      "Epoch 160/250\n",
      "898/898 [==============================] - 0s - loss: 0.0298 - acc: 0.9922 - val_loss: 0.1076 - val_acc: 0.9666\n",
      "Epoch 161/250\n",
      "898/898 [==============================] - 0s - loss: 0.0356 - acc: 0.9889 - val_loss: 0.1086 - val_acc: 0.9655\n",
      "Epoch 162/250\n",
      "898/898 [==============================] - 0s - loss: 0.0481 - acc: 0.9822 - val_loss: 0.1110 - val_acc: 0.9644\n",
      "Epoch 163/250\n",
      "898/898 [==============================] - 0s - loss: 0.0430 - acc: 0.9855 - val_loss: 0.1008 - val_acc: 0.9711\n",
      "Epoch 164/250\n",
      "898/898 [==============================] - 0s - loss: 0.0359 - acc: 0.9911 - val_loss: 0.1023 - val_acc: 0.9689\n",
      "Epoch 165/250\n",
      "898/898 [==============================] - 0s - loss: 0.0268 - acc: 0.9955 - val_loss: 0.1035 - val_acc: 0.9677\n",
      "Epoch 166/250\n",
      "898/898 [==============================] - 0s - loss: 0.0364 - acc: 0.9900 - val_loss: 0.1089 - val_acc: 0.9644\n",
      "Epoch 167/250\n",
      "898/898 [==============================] - 0s - loss: 0.0257 - acc: 0.9978 - val_loss: 0.1025 - val_acc: 0.9666\n",
      "Epoch 168/250\n",
      "898/898 [==============================] - 0s - loss: 0.0275 - acc: 0.9922 - val_loss: 0.1154 - val_acc: 0.9633\n",
      "Epoch 169/250\n",
      "898/898 [==============================] - 0s - loss: 0.0294 - acc: 0.9911 - val_loss: 0.1001 - val_acc: 0.9689\n",
      "Epoch 170/250\n",
      "898/898 [==============================] - 0s - loss: 0.0348 - acc: 0.9878 - val_loss: 0.0997 - val_acc: 0.9711\n",
      "Epoch 171/250\n",
      "898/898 [==============================] - 0s - loss: 0.0244 - acc: 0.9978 - val_loss: 0.1078 - val_acc: 0.9677\n",
      "Epoch 172/250\n",
      "898/898 [==============================] - 0s - loss: 0.0272 - acc: 0.9933 - val_loss: 0.1092 - val_acc: 0.9655\n",
      "Epoch 173/250\n",
      "898/898 [==============================] - 0s - loss: 0.0259 - acc: 0.9933 - val_loss: 0.1096 - val_acc: 0.9677\n",
      "Epoch 174/250\n",
      "898/898 [==============================] - 0s - loss: 0.0266 - acc: 0.9911 - val_loss: 0.1005 - val_acc: 0.9677\n",
      "Epoch 175/250\n",
      "898/898 [==============================] - 0s - loss: 0.0228 - acc: 0.9944 - val_loss: 0.0985 - val_acc: 0.9666\n",
      "Epoch 176/250\n",
      "898/898 [==============================] - 0s - loss: 0.0233 - acc: 0.9944 - val_loss: 0.1005 - val_acc: 0.9689\n",
      "Epoch 177/250\n",
      "898/898 [==============================] - 0s - loss: 0.0326 - acc: 0.9911 - val_loss: 0.0999 - val_acc: 0.9677\n",
      "Epoch 178/250\n",
      "898/898 [==============================] - 0s - loss: 0.0225 - acc: 0.9933 - val_loss: 0.1046 - val_acc: 0.9700\n",
      "Epoch 179/250\n",
      "898/898 [==============================] - 0s - loss: 0.0210 - acc: 0.9933 - val_loss: 0.1041 - val_acc: 0.9700\n",
      "Epoch 180/250\n",
      "898/898 [==============================] - 0s - loss: 0.0174 - acc: 0.9955 - val_loss: 0.1108 - val_acc: 0.9700\n",
      "Epoch 181/250\n",
      "898/898 [==============================] - 0s - loss: 0.0258 - acc: 0.9933 - val_loss: 0.1125 - val_acc: 0.9655\n",
      "Epoch 182/250\n",
      "898/898 [==============================] - 0s - loss: 0.0246 - acc: 0.9911 - val_loss: 0.0974 - val_acc: 0.9700\n",
      "Epoch 183/250\n",
      "898/898 [==============================] - 0s - loss: 0.0340 - acc: 0.9900 - val_loss: 0.1109 - val_acc: 0.9677\n",
      "Epoch 184/250\n",
      "898/898 [==============================] - 0s - loss: 0.0281 - acc: 0.9944 - val_loss: 0.0985 - val_acc: 0.9722\n",
      "Epoch 185/250\n",
      "898/898 [==============================] - 0s - loss: 0.0317 - acc: 0.9911 - val_loss: 0.1295 - val_acc: 0.9622\n",
      "Epoch 186/250\n",
      "898/898 [==============================] - 0s - loss: 0.0288 - acc: 0.9922 - val_loss: 0.1002 - val_acc: 0.9733\n",
      "Epoch 187/250\n",
      "898/898 [==============================] - 0s - loss: 0.0261 - acc: 0.9933 - val_loss: 0.1054 - val_acc: 0.9700\n",
      "Epoch 188/250\n",
      "898/898 [==============================] - 0s - loss: 0.0200 - acc: 0.9955 - val_loss: 0.0940 - val_acc: 0.9744\n",
      "Epoch 189/250\n",
      "898/898 [==============================] - 0s - loss: 0.0269 - acc: 0.9933 - val_loss: 0.0933 - val_acc: 0.9733\n",
      "Epoch 190/250\n",
      "898/898 [==============================] - 0s - loss: 0.0166 - acc: 0.9967 - val_loss: 0.0967 - val_acc: 0.9744\n",
      "Epoch 191/250\n",
      "898/898 [==============================] - 0s - loss: 0.0331 - acc: 0.9922 - val_loss: 0.1052 - val_acc: 0.9689\n",
      "Epoch 192/250\n",
      "898/898 [==============================] - 0s - loss: 0.0289 - acc: 0.9911 - val_loss: 0.1047 - val_acc: 0.9700\n",
      "Epoch 193/250\n",
      "898/898 [==============================] - 0s - loss: 0.0197 - acc: 0.9944 - val_loss: 0.0999 - val_acc: 0.9711\n",
      "Epoch 194/250\n",
      "898/898 [==============================] - 0s - loss: 0.0283 - acc: 0.9922 - val_loss: 0.0969 - val_acc: 0.9689\n",
      "Epoch 195/250\n",
      "898/898 [==============================] - 0s - loss: 0.0152 - acc: 0.9955 - val_loss: 0.0988 - val_acc: 0.9711\n",
      "Epoch 196/250\n",
      "898/898 [==============================] - 0s - loss: 0.0103 - acc: 0.9989 - val_loss: 0.0979 - val_acc: 0.9733\n",
      "Epoch 197/250\n",
      "898/898 [==============================] - 0s - loss: 0.0196 - acc: 0.9944 - val_loss: 0.1072 - val_acc: 0.9677\n",
      "Epoch 198/250\n",
      "898/898 [==============================] - 0s - loss: 0.0287 - acc: 0.9922 - val_loss: 0.1039 - val_acc: 0.9689\n",
      "Epoch 199/250\n",
      "898/898 [==============================] - 0s - loss: 0.0154 - acc: 0.9967 - val_loss: 0.0978 - val_acc: 0.9744\n",
      "Epoch 200/250\n",
      "898/898 [==============================] - 0s - loss: 0.0218 - acc: 0.9967 - val_loss: 0.1047 - val_acc: 0.9722\n",
      "Epoch 201/250\n",
      "898/898 [==============================] - 0s - loss: 0.0267 - acc: 0.9911 - val_loss: 0.0968 - val_acc: 0.9733\n",
      "Epoch 202/250\n",
      "898/898 [==============================] - 0s - loss: 0.0206 - acc: 0.9922 - val_loss: 0.0931 - val_acc: 0.9755\n",
      "Epoch 203/250\n",
      "898/898 [==============================] - 0s - loss: 0.0190 - acc: 0.9967 - val_loss: 0.1064 - val_acc: 0.9733\n",
      "Epoch 204/250\n",
      "898/898 [==============================] - 0s - loss: 0.0229 - acc: 0.9922 - val_loss: 0.1074 - val_acc: 0.9689\n",
      "Epoch 205/250\n",
      "898/898 [==============================] - 0s - loss: 0.0102 - acc: 0.9989 - val_loss: 0.1106 - val_acc: 0.9677\n",
      "Epoch 206/250\n",
      "898/898 [==============================] - 0s - loss: 0.0209 - acc: 0.9922 - val_loss: 0.1156 - val_acc: 0.9700\n",
      "Epoch 207/250\n",
      "898/898 [==============================] - 0s - loss: 0.0186 - acc: 0.9955 - val_loss: 0.1024 - val_acc: 0.9744\n",
      "Epoch 208/250\n",
      "898/898 [==============================] - 0s - loss: 0.0150 - acc: 0.9967 - val_loss: 0.0934 - val_acc: 0.9755\n",
      "Epoch 209/250\n",
      "898/898 [==============================] - 0s - loss: 0.0164 - acc: 0.9955 - val_loss: 0.0969 - val_acc: 0.9766\n",
      "Epoch 210/250\n",
      "898/898 [==============================] - 0s - loss: 0.0189 - acc: 0.9944 - val_loss: 0.1043 - val_acc: 0.9733\n",
      "Epoch 211/250\n",
      "898/898 [==============================] - 0s - loss: 0.0227 - acc: 0.9922 - val_loss: 0.0938 - val_acc: 0.9733\n",
      "Epoch 212/250\n",
      "898/898 [==============================] - 0s - loss: 0.0137 - acc: 0.9978 - val_loss: 0.0959 - val_acc: 0.9733\n",
      "Epoch 213/250\n",
      "898/898 [==============================] - 0s - loss: 0.0161 - acc: 0.9967 - val_loss: 0.1044 - val_acc: 0.9733\n",
      "Epoch 214/250\n",
      "898/898 [==============================] - 0s - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0929 - val_acc: 0.9733\n",
      "Epoch 215/250\n",
      "898/898 [==============================] - 0s - loss: 0.0168 - acc: 0.9955 - val_loss: 0.0948 - val_acc: 0.9744\n",
      "Epoch 216/250\n",
      "898/898 [==============================] - 0s - loss: 0.0122 - acc: 0.9978 - val_loss: 0.0964 - val_acc: 0.9733\n",
      "Epoch 217/250\n",
      "898/898 [==============================] - 0s - loss: 0.0146 - acc: 0.9944 - val_loss: 0.0956 - val_acc: 0.9744\n",
      "Epoch 218/250\n",
      "898/898 [==============================] - 0s - loss: 0.0169 - acc: 0.9944 - val_loss: 0.1173 - val_acc: 0.9655\n",
      "Epoch 219/250\n",
      "898/898 [==============================] - 0s - loss: 0.0217 - acc: 0.9922 - val_loss: 0.1033 - val_acc: 0.9711\n",
      "Epoch 220/250\n",
      "898/898 [==============================] - 0s - loss: 0.0235 - acc: 0.9944 - val_loss: 0.1049 - val_acc: 0.9722\n",
      "Epoch 221/250\n",
      "898/898 [==============================] - 0s - loss: 0.0120 - acc: 0.9978 - val_loss: 0.1021 - val_acc: 0.9744\n",
      "Epoch 222/250\n",
      "898/898 [==============================] - 0s - loss: 0.0096 - acc: 0.9967 - val_loss: 0.1014 - val_acc: 0.9755\n",
      "Epoch 223/250\n",
      "898/898 [==============================] - 0s - loss: 0.0129 - acc: 0.9967 - val_loss: 0.1051 - val_acc: 0.9744\n",
      "Epoch 224/250\n",
      "898/898 [==============================] - 0s - loss: 0.0122 - acc: 0.9967 - val_loss: 0.1125 - val_acc: 0.9733\n",
      "Epoch 225/250\n",
      "898/898 [==============================] - 0s - loss: 0.0138 - acc: 0.9967 - val_loss: 0.1123 - val_acc: 0.9700\n",
      "Epoch 226/250\n",
      "898/898 [==============================] - 0s - loss: 0.0160 - acc: 0.9944 - val_loss: 0.1004 - val_acc: 0.9722\n",
      "Epoch 227/250\n",
      "898/898 [==============================] - 0s - loss: 0.0153 - acc: 0.9955 - val_loss: 0.1039 - val_acc: 0.9733\n",
      "Epoch 228/250\n",
      "898/898 [==============================] - 0s - loss: 0.0147 - acc: 0.9967 - val_loss: 0.0976 - val_acc: 0.9755\n",
      "Epoch 229/250\n",
      "898/898 [==============================] - 0s - loss: 0.0112 - acc: 0.9989 - val_loss: 0.1016 - val_acc: 0.9733\n",
      "Epoch 230/250\n",
      "898/898 [==============================] - 0s - loss: 0.0142 - acc: 0.9944 - val_loss: 0.0996 - val_acc: 0.9744\n",
      "Epoch 231/250\n",
      "898/898 [==============================] - 0s - loss: 0.0160 - acc: 0.9933 - val_loss: 0.0965 - val_acc: 0.9755\n",
      "Epoch 232/250\n",
      "898/898 [==============================] - 0s - loss: 0.0129 - acc: 0.9967 - val_loss: 0.0971 - val_acc: 0.9755\n",
      "Epoch 233/250\n",
      "898/898 [==============================] - 0s - loss: 0.0105 - acc: 0.9978 - val_loss: 0.0990 - val_acc: 0.9733\n",
      "Epoch 234/250\n",
      "898/898 [==============================] - 0s - loss: 0.0162 - acc: 0.9967 - val_loss: 0.1038 - val_acc: 0.9711\n",
      "Epoch 235/250\n",
      "898/898 [==============================] - 0s - loss: 0.0163 - acc: 0.9955 - val_loss: 0.0988 - val_acc: 0.9744\n",
      "Epoch 236/250\n",
      "898/898 [==============================] - 0s - loss: 0.0124 - acc: 0.9944 - val_loss: 0.0989 - val_acc: 0.9700\n",
      "Epoch 237/250\n",
      "898/898 [==============================] - 0s - loss: 0.0192 - acc: 0.9955 - val_loss: 0.1032 - val_acc: 0.9722\n",
      "Epoch 238/250\n",
      "898/898 [==============================] - 0s - loss: 0.0081 - acc: 0.9989 - val_loss: 0.0922 - val_acc: 0.9733\n",
      "Epoch 239/250\n",
      "898/898 [==============================] - 0s - loss: 0.0092 - acc: 0.9989 - val_loss: 0.0945 - val_acc: 0.9733\n",
      "Epoch 240/250\n",
      "898/898 [==============================] - 0s - loss: 0.0117 - acc: 0.9989 - val_loss: 0.1051 - val_acc: 0.9722\n",
      "Epoch 241/250\n",
      "898/898 [==============================] - 0s - loss: 0.0225 - acc: 0.9944 - val_loss: 0.1098 - val_acc: 0.9722\n",
      "Epoch 242/250\n",
      "898/898 [==============================] - 0s - loss: 0.0179 - acc: 0.9922 - val_loss: 0.1012 - val_acc: 0.9722\n",
      "Epoch 243/250\n",
      "898/898 [==============================] - 0s - loss: 0.0132 - acc: 0.9944 - val_loss: 0.1069 - val_acc: 0.9711\n",
      "Epoch 244/250\n",
      "898/898 [==============================] - 0s - loss: 0.0156 - acc: 0.9967 - val_loss: 0.1131 - val_acc: 0.9722\n",
      "Epoch 245/250\n",
      "898/898 [==============================] - 0s - loss: 0.0157 - acc: 0.9978 - val_loss: 0.0962 - val_acc: 0.9733\n",
      "Epoch 246/250\n",
      "898/898 [==============================] - 0s - loss: 0.0255 - acc: 0.9967 - val_loss: 0.1021 - val_acc: 0.9722\n",
      "Epoch 247/250\n",
      "898/898 [==============================] - 0s - loss: 0.0135 - acc: 0.9967 - val_loss: 0.1049 - val_acc: 0.9677\n",
      "Epoch 248/250\n",
      "898/898 [==============================] - 0s - loss: 0.0129 - acc: 0.9955 - val_loss: 0.1016 - val_acc: 0.9733\n",
      "Epoch 249/250\n",
      "898/898 [==============================] - 0s - loss: 0.0139 - acc: 0.9967 - val_loss: 0.1070 - val_acc: 0.9722\n",
      "Epoch 250/250\n",
      "898/898 [==============================] - 0s - loss: 0.0109 - acc: 0.9989 - val_loss: 0.0991 - val_acc: 0.9722\n"
     ]
    }
   ],
   "source": [
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train.reshape(len(x_train),img_rows,img_cols,1),\n",
    "          keras.utils.to_categorical(y_train, num_classes),\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 1,\n",
    "          validation_data = (x_test.reshape(len(x_test),img_rows,img_cols,1),\n",
    "                             keras.utils.to_categorical(y_test, num_classes))\n",
    "         )\n",
    "\n",
    "score = model.evaluate(x_test.reshape(len(x_test),img_rows,img_cols,1),\n",
    "                       keras.utils.to_categorical(y_test, num_classes),\n",
    "                       verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/899 [=================>............] - ETA: 0s\n",
      "\n",
      "\n",
      "Classification Report for Convolutional Neural Network Classifier: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        88\n",
      "          1       0.99      0.96      0.97        91\n",
      "          2       0.98      1.00      0.99        86\n",
      "          3       0.98      0.92      0.95        91\n",
      "          4       0.99      0.93      0.96        92\n",
      "          5       0.96      0.99      0.97        91\n",
      "          6       0.98      1.00      0.99        91\n",
      "          7       0.97      0.99      0.98        89\n",
      "          8       0.98      0.98      0.98        88\n",
      "          9       0.92      0.98      0.95        92\n",
      "\n",
      "avg / total       0.97      0.97      0.97       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_pred = model.predict_classes(x_test.reshape(len(x_test),img_rows,img_cols,1))\n",
    "\n",
    "print(\"\\n\\n\\nClassification Report for Convolutional Neural Network Classifier: \\n%s\\n\"\n",
    "      % (metrics.classification_report(y_test, cnn_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
